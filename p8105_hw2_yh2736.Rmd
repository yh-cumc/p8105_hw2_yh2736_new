---
title: "P8105 Homework2"
author: "Yongmei Huang"
date: "10/4/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(lubridate)

```


# Problem 1
```{r, trashwheel, echo = TRUE}
##upload and clean the trashwheel dataset
trashwheel_dataset = read_excel(path = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                                sheet = "Mr. Trash Wheel",
                                range = cell_cols("A:N")) %>%  ## upload the xlsx file
  janitor::clean_names() %>%                                   ## clean the dataset
  na.omit()                                                    ## omit row with NULL data    

trashwheel_dataset$sports_balls = round(as.integer(trashwheel_dataset$sports_balls), 
                                        digits = 0)     ## round the sports balls to nearest inter
####checkpoints####
count(trashwheel_dataset) #344 obs#
head(trashwheel_dataset,5)
summary(trashwheel_dataset)
####checkend####

```

```{r, precipitation2017, echo = TRUE}
##upload and clean the precipitation data of 2017
precipitation_dataset_2017 = read_excel(path = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                                        sheet = "2017 Precipitation", 
                                        range = "A2:B14") %>%  ##upload 2017 precipitation data 
  na.omit() %>%                                                  ##omit rows without precipitation data
  janitor::clean_names() %>%                                     ##clean data
  mutate(year = '2017')                                          ##add a variable year

####checkpoints####
count(precipitation_dataset_2017) #12 obs#
view(precipitation_dataset_2017)
summary(precipitation_dataset_2017)
####checkend####

```

```{r, precipitation2018, echo = TRUE}
##upload and clean the precipitation data of 2017
precipitation_dataset_2018 = read_excel(path = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                                        sheet = "2018 Precipitation", 
                                        range = "A2:B14") %>%    ##upload 2018 precipitation data
  na.omit() %>%                                                  ##omit rows without precipitation data
  janitor::clean_names() %>%                                     ##clean data
  mutate(year = '2018')                ##add a variable year


####checkpoints####
count(precipitation_dataset_2018) #12 obs#
view(precipitation_dataset_2018)
summary(precipitation_dataset_2018)
####checkend####

```

```{r, combindataset, echo = TRUE}
##combination the two dataset of precipitation_dataset_2017 and precipitation_dataset_2018
precipitation_dataset = left_join(
  precipitation_dataset_2017, 
  precipitation_dataset_2018, 
  by = "month",
  suffix = c("_2017", "_2018")
) %>% 
  select("month", "total_2017", "total_2018") %>% 
  rename("precipitation_2017" = "total_2017", "precipitation_2018" = "total_2018") 
precipitation_dataset$month = month.name[precipitation_dataset$month]

####checkpoints####
count(precipitation_dataset) #12 obs#
view(precipitation_dataset)
summary(precipitation_dataset)
####checkend####

```

In problem1, three datasets were used. Trash wheel includes information of 344 dumpster from year 2014 to 2019. For each dumpster, the dataset collects information on weight, volume, plastic bottles, polystyrene, cigarette butts, glass bottles, grocery bags, chip bags, sports balls, and homes powered.Each of these information is a independent varaible that can be summaried using the commamment of summary() that is presented in the my command line as check points. For example,the collected sport balls range from 0 to 56 with a median of 8 and IQR 5-16. The sport ball data is right skwed for the mean (11.79) is much bigger than the median (8). 

In the original Trash Wheel Collection excel sheet, the precipitation was collected seperately by year. In the assigment, we are requested to upload 2017 and 2018 data, and combine these two year data. For the convenience of data revievewing, I use the left join for the wide format of dataset, and create seperate variables to represent the precipitation by year in each month. The combination can also be done in long format by stacking 2017 data on the top of 2018 data and remaining year as a variable. The wide format is my final option that shows a better view. Using the summary command, we know that the median value of precipitation was 2.145(IQR 1.285-4.103) in 2017 and 5.455(IQR 4.190-8.182). The precipitation in 2017 is much higher than that in 2018. The distribution of precipitation data in both years are closer to normal distrubuted than the sport ball data in trasj wheel if we compare the difference of meidans and means.        


# Problem 2
```{r, pols, echo = TRUE}
##upload and clean data in pols-month.csv
pols_month_dataset = read_csv(file = "./data/pols-month.csv") %>%   ##upload pols-month.csv
  janitor::clean_names() %>% ##clean the dataset
  separate(col = "mon", 
           sep = "-", 
           into = c("year", "month", "day"), 
           convert = type.convert("month", "year", "day" , 
                                  numerals = c("allow.loss"))) %>%  
           ##separate to year, month, day, and transfer month format "01" "02"... to "1", "2)
  transform(month = month.abb[month]) %>%       ##replace month number with month name
  mutate(president = case_when(
    prez_dem == 1 ~ "dem",
    prez_gop == 1 ~ "gop",
    TRUE ~ ""
  )) %>%  ##add variable president with value "dem" and "gop"
  select("year", "month", "gov_gop":"rep_gop", "gov_dem":"rep_dem", "president") 
   ##remove prez_gop, prez_dem and day variable

####checkpoints####
count(pols_month_dataset) # 822 observations #
head(pols_month_dataset,20)
summary(pols_month_dataset)
sum(is.na(close))
####checkend####

```


```{r, snp, echo = TRUE}
##upload and clean data in snp.csv
snp_dataset = read_csv(file = "./data/snp.csv") %>% ##upload the data
  janitor::clean_names() %>%                         ##clean the data
  separate(col = "date", 
           sep = "/", 
           into = c("month", "day", "year"), 
           convert = type.convert("month", "day", "year", 
                                  numerals = c("allow.loss"))) %>% 
  transform(month = month.abb[month]) %>%            ##separate the data
  select("year", "month", "day", everything())       ##arrange the data

####checkpoints####
count(snp_dataset) # 787 observations #
head(snp_dataset,20)
summary(snp_dataset)
sum(is.na(close))
####checkend####

```

```{r, unemployment, echo = TRUE}
##upload and clean unemployment data
unemployment_dataset = read_csv(file = "./data/unemployment.csv") %>% 
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment"
  ) %>% 
  janitor::clean_names()          ##clean the data

####checkpoints####
count(unemployment_dataset) # 816 observations #
head(unemployment_dataset,20)
summary(unemployment_dataset)
sum(is.na(unemployment_dataset$unemployment)) 
# 6 missing value of unemployment in year 2015 from July to December#
####checkend####

```

```{r, merg_pol_snp_unemplm, echo = TRUE}
## first merge the snp and pols dataset
merg_snp_pols_unem_dataset = full_join(snp_dataset, 
                                  pols_month_dataset, 
                                  by = c("year", "month")) %>% 
  left_join(unemployment_dataset, by = c("year", "month"))   ## then merge pols, snp and unemployment data

####checkpoints####
count(merg_snp_pols_unem_dataset) # 823 observations #
head(merg_snp_pols_unem_dataset,20)
summary(merg_snp_pols_unem_dataset)
sum(is.na(merg_snp_pols_unem_dataset$unemployment)) #missing data 13#
####checkend####
```

In problem2, we look at three datasets and link all of them togethere. The poll dataset has 822 observarions from year 1947 to 2015, including poll date (year, month, day), monthly number of governors, senates,representatives, and presidents from the party of republic or democratic with variable names of gov-pop, sen_gop, rep_gop,gov_dem, sen_dem, rep_dem, and president. As per question2 requirement, we create a new varaible "president" as character variable to indicate the party of the president: democratic or republic. 

The dataset "snp" has 787 observations and 2 variables, indicating the closing values of the S&P index in each corresponding date. The median value of closing S&P is 138.58 (IQR:83.73-941.79). The mean is 474.89. Since the mean value of S&P is far greater than median, the S&P is right skewed. 

The dataset "unemployment" has 816 observations and 2 variables, indicating unemployment rate in each month from year 1948 to 2015. The median unemployment rate was 5.6% (IQR: 4.7-6.9%). The highest rate was 10.8% and the lowest was 2.5%. In year 2015 from JUly to December, the unemployment data was missing. 

We, then, link all three individual datase togehter in two steps. 












# Problem 3
```{r, babyname, echo = TRUE}
#upload and clean the data
popular_baby_name_dataset = read_csv(file = "./data/Popular_Baby_Names.csv") %>% 
  janitor::clean_names() %>% 
  distinct()

##filter the male baby with "white non hispanic" and "2016"
popular_male_baby_name_dataset = popular_baby_name_dataset %>% 
  filter(gender == "MALE") %>% 
  filter(ethnicity == "WHITE NON HISPANIC") %>% 
  filter(year_of_birth == 2016)

##produce a scatter plot showing the number of children with a name (y axis) against the rank in popularity of that name (x axis)
plot(popular_male_baby_name_dataset$rank, popular_male_baby_name_dataset$count,
     xlab = "the number of children with a name",
     ylab = "the rank in popularity of that name")


```

